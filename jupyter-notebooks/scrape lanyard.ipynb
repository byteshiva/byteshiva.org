{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import lxml.html\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "URL = \"http://lanyrd.com/profile/jacobian/coverage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the dates that the wayback machine has.\n",
    "\n",
    "resp = requests.get('https://web.archive.org/cdx/search/cdx', params={\n",
    "    \"url\": URL,\n",
    "    \"showDupeCount\": \"true\",\n",
    "    \"output\": \"json\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a weird data structure -- a list of lists, the first item being headers \n",
    "# and the remaining being fields. Sorta like CSV encoded into JSON... so munge that \n",
    "# into a better format:\n",
    "\n",
    "cdx = resp.json()\n",
    "fields = cdx[0]\n",
    "snapshots = [dict(zip(fields, row)) for row in cdx[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_archive_url(url):\n",
    "    \"\"\"\n",
    "    Convert an archive.org URL (https://web.archive.org/web/ddddd/URL) to the original.\n",
    "    \"\"\"\n",
    "    return re.sub(\".*/web/\\d+/(.*)\", r\"\\1\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS_3_REV = {\"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4, \"may\": 5, \"jun\": 6,\n",
    "                \"jul\": 7, \"aug\": 8, \"sep\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12}\n",
    "\n",
    "def extract_date(s):\n",
    "    \"\"\"\n",
    "    Extract a date from lanyrd-style date string\n",
    "    \n",
    "    This can be a range (e.g. \"3rd-8th June 2018\")\n",
    "    or a single date (\"3rd June 2018\").\n",
    "    \n",
    "    FIXME: what if it spans a month?\n",
    "    \n",
    "    This fucntion always returns (start, end), even\n",
    "    if it's just a single date.\n",
    "    \"\"\"\n",
    "    # Two-date version:\n",
    "    m = re.search(r'(\\d{1,2})(th|st|rd)-(\\d{1,2})(th|st|rd) (\\w+) (\\d{4})', s)\n",
    "    if m:\n",
    "        start_day, _, end_day, _, month, year = m.groups()\n",
    "        month = MONTHS_3_REV[month.lower()[:3]]\n",
    "        start_date = datetime.date(int(year), month, int(start_day))\n",
    "        end_date = datetime.date(int(year), month, int(end_day))\n",
    "        return (start_date, end_date)\n",
    "    \n",
    "    # Single-date version\n",
    "    m = re.search(r'(\\d{1,2})(th|st|rd) (\\w+) (\\d{4})', s)\n",
    "    if m:\n",
    "        day, _, month, year = m.groups()\n",
    "        month = MONTHS_3_REV[month.lower()[:3]]\n",
    "        date = datetime.date(int(year), month, int(day))\n",
    "        return (date, date)\n",
    "\n",
    "    raise ValueError(f\"can't parse date string: '{s}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_coverage(snapshots):\n",
    "    \"\"\"\n",
    "    Scrape coverage (video, slides, etc) from each snapshot\n",
    "    Have to do this because pagination isn't archived (it\n",
    "    uses query strings), so this is the best way to go back\n",
    "    and gather as much as possible\n",
    "    \"\"\"\n",
    "    coverage = {}\n",
    "\n",
    "    # Map conference titles to conference info, so that we don't\n",
    "    # fetch it multiple times for repeat conferences\n",
    "    conferences = {}\n",
    "\n",
    "    for snapshot in snapshots:\n",
    "        ts = snapshot[\"timestamp\"]\n",
    "        resp = requests.get(f\"https://web.archive.org/web/{ts}/{URL}\")\n",
    "        doc = lxml.html.fromstring(resp.text)\n",
    "        for ci in doc.cssselect('#coverage .coverage-item'):\n",
    "\n",
    "            # Extract link to the coverage (view, slides, etc)\n",
    "            # Treat this as the unique id since as we page we might\n",
    "            # see these repeatedly. Skip if we've seen it before.\n",
    "            coverage_link = strip_archive_url(ci.cssselect('.title a')[0].attrib['href'])\n",
    "            if coverage_link in coverage:\n",
    "                continue\n",
    "\n",
    "            coverage_type = ci.cssselect('span.type')[0].text.lower()\n",
    "\n",
    "            # Talk and conference title, and links to the archive.org pages\n",
    "            # Don't follow these now, since the same conference and talk\n",
    "            # may show up multiple times.\n",
    "            e_talk, e_conference, *_ = ci.cssselect('p.meta a')\n",
    "            talk_title = e_talk.text\n",
    "\n",
    "            con_title = e_conference.text\n",
    "            if con_title not in conferences:\n",
    "                conferences[con_title] = scrape_conference(e_conference.attrib['href'])\n",
    "\n",
    "            start_date, end_date = extract_date(ci.cssselect('p.meta')[0].text_content())\n",
    "\n",
    "            coverage[coverage_link] = {\n",
    "                'type': coverage_type,\n",
    "                'talk_title': talk_title,\n",
    "                'conference_title': con_title,\n",
    "                'conference': conferences[con_title],\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date\n",
    "            }\n",
    "            \n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_conference(url):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        return {'error': resp.status_code}\n",
    "        \n",
    "    doc = lxml.html.fromstring(resp.text)\n",
    "    \n",
    "    return {\n",
    "        'title': doc.cssselect('h1.summary')[0].text_content(),\n",
    "        'link': strip_archive_url(doc.cssselect('a.icon.url.website')[0].attrib['href'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = scrape_coverage(snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_dates(obj):\n",
    "    if hasattr(obj, 'isoformat'):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(type(obj))\n",
    "        \n",
    "with open('/tmp/lanyrd-coverage.json', 'w') as fp:\n",
    "    json.dump(coverage, fp, default=serialize_dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
